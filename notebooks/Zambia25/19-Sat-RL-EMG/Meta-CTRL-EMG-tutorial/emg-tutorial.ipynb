{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbdc540c-6ac6-4f34-b9b9-b61538184074",
   "metadata": {},
   "source": [
    "## ðŸ”„ Code Attribution & Modification Notice\n",
    "\n",
    "Code originally developed by **Jorge A. Menendez**, (Research Scientist, Meta EMG Research Team) for TReND CaMinA 2024  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26420dc-318d-438f-ab38-1e0ed83ed6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please run this notebook locally. The data file takes too long to upload on Colab.\n",
    "# You can work on a clone of the TRenD-CaMinA repository on your laptop (preferable) or\n",
    "# alternatively, download the whole tutorial folder Meta-CTRL-EMG-tutorial to your laptop.\n",
    "# Then run the notebook locally by typing in the terminal:\n",
    "# jupyter notebook emg-tutorial.ipynb\n",
    "\n",
    "# Don't forget to download the data data.npz from our googledrive and place it within the tutorial folder\n",
    "# Meta-CTRL-EMG-tutorial\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns # If you don't have seaborn, instally locally with: pip install seaborn\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60dea58-b56a-42ac-8950-31c77638e91a",
   "metadata": {},
   "source": [
    "#### What is this tutorial about?\n",
    "\n",
    "This tutorial covers three broad goals:\n",
    "1. dig into what surface EMG signals typically look like, and how the signal features reflect underlying physiology\n",
    "2. motivate and demonstrate the use of standard signal processing techniques to enable the extraction of behavioral information from the surface EMG signal\n",
    "3. demonstrate the use of simple machine learning tools to extract this information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48567d22-30e2-4401-89ab-55758bd5610c",
   "metadata": {},
   "source": [
    "## 1. The data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f4a29a-0119-470f-921b-1a4b534b7dd3",
   "metadata": {},
   "source": [
    "### 1.1. Surface EMG\n",
    "\n",
    "To this end, we will analyze some recently published EMG data from [Harshavardhana & Miller (2024)](https://iopscience.iop.org/article/10.1088/1741-2552/ad5107/). In this study, the authors collected surface EMG data using 12 electrodes placed around the wrist and around the forearm, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608405b3-7b11-417a-a057-6141f61219b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"./electrodes.png\", width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bb269b-6e6d-4565-8f08-da19ecafefff",
   "metadata": {},
   "source": [
    "We refer to each of these sensors as electrode 0, electrode 1, ..., electrode 11, using 0-based indexing to match the same indexing we use to index them in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8620086c-dc02-4a37-b0f2-3d1dd961673c",
   "metadata": {},
   "source": [
    "### 1.2. Behaviors\n",
    "\n",
    "While EMG was recorded, subjects performed the following 10 hand gestures 36 times each, in a random order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef71791-1687-4f30-bc89-460aedd75552",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"./gestures.png\", width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478a9d18-7a82-42ff-a4b6-2c6830c889ce",
   "metadata": {},
   "source": [
    "### 1.3. Our goal\n",
    "\n",
    "Our goal in this tutorial is to build a *gesture decoder*, which automatically detects from the EMG signal which of these 10 gestures was performed by the subject. Such a decoder could be used for a neuromotor human-computer interface with 10 buttons in which you could \"click\" each of these buttons by simply producing the corresponding gesture, allowing the EMG sensors to detect your intent, rather than some physical button or controller.\n",
    "\n",
    "Below, we will analyze the data from one subject from this experiment. We will dig into some of their data in detail to examine and understand the various sources of variability in EMG, and then try to use our insights to build and evaluate a simple gesture decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a70d42-2c2f-4dae-94ad-31944c30c578",
   "metadata": {},
   "source": [
    "### 1.4. Load and inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c311d354-055d-4265-88df-51375aba545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Constants indicating where to load the data from\n",
    "and the gesture names corresponding to each label\n",
    "\"\"\"\n",
    "\n",
    "URL = \"https://osf.io/download/q7f4x/\" # we have downloaded the data for you\n",
    "FILENAME = \"data.npz\" # you can find this file on our googledrive\n",
    "# Download the data.npz file from the googledrive and place it within the code folder\n",
    "# Meta-CTRL-EMG-tutorial\n",
    "\n",
    "GESTURE_NAMES = [\n",
    "    \"Down\",\n",
    "    \"Index finger pinch\",\n",
    "    \"Left\",\n",
    "    \"Middle finger pinch\",\n",
    "    \"Index point\",\n",
    "    \"Power grasp\",\n",
    "    \"Right\",\n",
    "    \"Two finger pinch\",\n",
    "    \"Up\",\n",
    "    \"Splay\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794a2e09-9c27-4570-9f39-f340d7d3bfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Helper functions to load the data\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "\n",
    "def download_data_from_url(url: str, filename: str) -> None:\n",
    "    \"\"\"Download data froma given URL to a local file\"\"\"\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        with open(filename, \"wb\") as fid:\n",
    "            fid.write(r.content)\n",
    "    except requests.ConnectionError:\n",
    "        print(\"!!! Failed to download data !!!\")\n",
    "\n",
    "def load_data() -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Load a single dataset from OSF\"\"\"\n",
    "    \n",
    "    # download_data_from_url(URL, FILENAME)\n",
    "    data = np.load(FILENAME)\n",
    "    \n",
    "    emg = data['DATA']\n",
    "    emg = emg.transpose([0, 2, 1])  # transpose from [trials, electrodes, time] -> [trials, time, electrodes]\n",
    "    \n",
    "    labels = data['LABELS']\n",
    "    labels = labels.astype(int)\n",
    "    labels = np.array(GESTURE_NAMES)[labels]  # get gesture name corresponding to each integer label\n",
    "    \n",
    "    return emg, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb03f25-1fb8-4236-b9f6-91c9239f180e",
   "metadata": {},
   "source": [
    "We load the data into two variables, called `emg` and `labels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852a1980-9bc5-454f-bdf4-520c9f82a7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "emg, labels = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703d46ed-5299-4906-a90f-807a3329f4ae",
   "metadata": {},
   "source": [
    "We first inspect the `emg` array by printing its shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919728a7-fb50-46cf-9ca4-b98428ffe9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials, num_samples, num_electrodes = emg.shape\n",
    "\n",
    "print(f\"emg shape: {num_trials} trials x {num_samples} samples x {num_electrodes} electrodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d31bc9-0217-4442-93a9-707795540ba7",
   "metadata": {},
   "source": [
    "This variable contains 360 trials, each with 4000 samples of EMG (2 seconds at 2000Hz sampling rate), from each of the 12 electrodes. Each one of these sequences corresponds to a different trial in which the subject was instructed to perform one of the 10 gestures.\n",
    "\n",
    "Which gesture was produced in each trial is given by the `labels` array, which contains 360 strings indicating the gesture produced in each of the 360 trials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1959faa3-05f6-491e-a0dd-b095af68645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"labels shape: \", labels.shape)\n",
    "print(\"labels values: \", set(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd866f55-e837-4f13-9185-dffcf5060298",
   "metadata": {},
   "source": [
    "## 2. Visualize the raw EMG signal\n",
    "\n",
    "Next, we'll pick a few of these trials and visualize the raw EMG signal. We'll try to relate the structure we see to the principles of EMG that were discussed in the lecture, in particular its composition in terms of motor unit action potentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c877a93f-09a9-4674-8b4e-e4a12a2b8631",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Helper functions for plotting\"\"\"\n",
    "\n",
    "from matplotlib.colors import Colormap\n",
    "\n",
    "EMG_SAMPLING_RATE_HZ = 2000  # see paper\n",
    "\n",
    "def plot_stacked_signals(\n",
    "    stacked_signals: np.ndarray,\n",
    "    xaxis: np.ndarray | None = None,\n",
    "    labels_prefix: str | None = None,\n",
    "    cmap: str | Colormap = \"gist_rainbow\",\n",
    "    spacing: float = 0.3,\n",
    "    yticks: bool = False,\n",
    "    ax: plt.Axes | None = None,\n",
    "    **kwargs,\n",
    "):\n",
    "    ax = ax or plt.gca()\n",
    "\n",
    "    if isinstance(cmap, str):\n",
    "        cmap = plt.get_cmap(cmap)\n",
    "\n",
    "    num_signals, _ = stacked_signals.shape\n",
    "\n",
    "    if labels_prefix is None:\n",
    "        labels = [None,] * num_signals\n",
    "    else:\n",
    "        labels = [f\"{labels_prefix} {i}\" for i in range(num_signals)]\n",
    "    \n",
    "    for i, (x, label) in enumerate(zip(stacked_signals, labels)):\n",
    "        ax.plot(\n",
    "            np.arange(len(x)) if xaxis is None else xaxis,\n",
    "            x + i * spacing,\n",
    "            color=cmap(i / (num_signals - 1)),\n",
    "            label=label,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    ax.set_yticks(np.arange(num_signals) * spacing)\n",
    "    ax.grid(axis=\"y\", which=\"major\", color=\"0.75\", zorder=-1)\n",
    "\n",
    "    if yticks:\n",
    "        ax.set_yticklabels(np.arange(num_signals))\n",
    "\n",
    "    return ax\n",
    "\n",
    "def plot_emg(\n",
    "    emg: np.ndarray,\n",
    "    cmap: str | Colormap = \"gist_rainbow\",\n",
    "    spacing: float = 0.3,\n",
    "    sampling_rate: float = EMG_SAMPLING_RATE_HZ,\n",
    "    ax: plt.Axes | None = None,\n",
    "    **kwargs,\n",
    "):\n",
    "    num_electrodes, num_tsteps = emg.shape\n",
    "    time = np.arange(num_tsteps) / sampling_rate\n",
    "    ax = plot_stacked_signals(\n",
    "        emg,\n",
    "        xaxis=time,\n",
    "        labels_prefix=\"electrode\",\n",
    "        cmap=cmap,\n",
    "        spacing=spacing,\n",
    "        yticks=True,\n",
    "        ax=ax,\n",
    "        **kwargs\n",
    "    )\n",
    "    sns.despine(ax=ax, left=True)\n",
    "    ax.set_ylabel(\"EMG electrode\")\n",
    "    ax.set_xlabel(\"time (sec)\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66298ccd-2c66-4ec1-9fcc-9fcb102886de",
   "metadata": {},
   "source": [
    "### 2.1. Inpsect raw EMG from one trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4808cff0-f62c-497e-bf68-95b337abd0d8",
   "metadata": {},
   "source": [
    "Lets first pick one trial and visualize the raw EMG signal in that trial. We pick the first trial of the \"Splay\" gesture, which is a sequence of 4000 EMG samples from 12 electrodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c546f1d-c43b-49f7-86b6-b21c45d6dc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_trial_emg(\n",
    "    data: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    gesture_label: str,\n",
    "    trial_index: int,\n",
    ") -> np.ndarray:\n",
    "    gesture_trial_indices, = np.where(labels == gesture_label)\n",
    "    return data[gesture_trial_indices[trial_index]]\n",
    "\n",
    "gesture = \"Splay\"\n",
    "trial = 0\n",
    "\n",
    "single_trial_emg = get_single_trial_emg(emg, labels, gesture, trial)\n",
    "\n",
    "print(f\"grabbing EMG data from one trial of the {gesture} gesture\")\n",
    "print(\"\\tdata shape:\", single_trial_emg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53650fb8-f395-49bf-888c-93a91e1e74c8",
   "metadata": {},
   "source": [
    "Next, we visualize it by plotting the EMG from each electrode stacked vertically: we start with electrode 0 at the bottom up to electrode 11 at the top, and color each electrode's data differently for the sake of visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669c6913-d56d-4603-bf38-ee5dbdb0f1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "plot_emg(single_trial_emg.T, ax=ax)\n",
    "\n",
    "ylim = ax.get_ylim()\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "snippet_times = [1.2525, 1.5225, 1.8175]\n",
    "snippet_length_sec = .015  # 15ms\n",
    "\n",
    "for t in snippet_times:\n",
    "    ax.fill_between(\n",
    "        [t, t + snippet_length_sec],\n",
    "        *ylim,\n",
    "        color=\"0.8\",\n",
    "        zorder=-1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d856f790-d3ad-486a-bfea-ce5d16838b79",
   "metadata": {},
   "source": [
    "Our first observation is that there is relatively little change in the EMG signal at the beginning of the trial, but about at half way many electrodes start lighting up. That's probably because that's when the user actually started producing the gesture, so the **muscles became active and started generating electrical activity**, which the sensors on the surface are able to pick up on.\n",
    "\n",
    "Our second observation is that this electrical activity is very \"spiky\": in some electrodes it appears like there are very sharp and fast deflections of the signal producing **rapid upward and downward peaks in activity**. A few of these are highlighted by the three gray shaded regions, which mark some prominant spikes in electrodes 9, 10, and 0, respectively, from left to right. These are **motor unit action potentials** being detected on the surface.\n",
    "\n",
    "Below, we zoom in and plot the EMG within those three shaded areas to see what these waveforms look like in more detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af1f408-e797-432e-aa59-977f5a712758",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    1, len(snippet_times),\n",
    "    figsize=(8, 4),\n",
    "    sharex=False,\n",
    "    sharey=True,\n",
    "    gridspec_kw=dict(wspace=0.5),\n",
    ")\n",
    "\n",
    "for i, (t, ax) in enumerate(zip(snippet_times, axes)):\n",
    "    plot_emg(single_trial_emg.T, ax=ax)\n",
    "    ax.set_xlim([t, t + snippet_length_sec])\n",
    "    ax.tick_params(axis=\"y\", length=0)\n",
    "    if i > 0:\n",
    "        ax.set_ylabel(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62a7fd7-418b-4498-ae19-c949479d7854",
   "metadata": {},
   "source": [
    "Within this very shot timesapce of .015 sec = 15 ms, we see that only a few electrodes show significant activity. However, that activity tends to be **spatially localized**: activity is typically shared by neighboring electrodes, not randomly distributed along the electrodes. In the leftmost and middle examples, only electrodes 6-7 and 9-10 are highly active; in the rightmost example electrodes 0-3 and 9-10.\n",
    "\n",
    "The reason for this is that the a single motor unit action potential within the muscular tissue **produces an electrical signal that emanates to a wider region of the skin** as it is conducted through the intervening biological tissue. It thus gets picked up on by the multiple EMG electrodes in that area. The more electrodes you have in that area, the better the picture you get of what that electrical signature looks like.\n",
    "\n",
    "Note as well that, even though neighboring electrodes are typically active at the same time, **the shape of that activity can be quite different in each electrode**. For example, in the EMG snippet on the right we see that both electrode 0 and electrode 1 detect something at the same time, but that event manifests itself as a positive peak in electrode 1 and a negative peak in electrode 0.\n",
    "\n",
    "The reason for this is that the electrical signature of the motor unit action potential can get distorted in different ways as it is conducted to the surface of the skin, depending on what tissue it has to traverse along the way. So even though the same motor unit action potential will typically be picked up by multiple nearby EMG electrodes, each of those electrodes may get a slightly different \"view\" of it depending on exactly where that electrode is placed on the skin relative to the motor unit's location beneath the skin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed2fad8-4fa6-4ff7-89db-9be6e3d279c4",
   "metadata": {},
   "source": [
    "### 2.2. Compare EMG evoked by different gestures\n",
    "\n",
    "Next, let's zoom back out a bit and look at the EMG signal evoked by each gesture, and ask how it differs across gestures.\n",
    "\n",
    "Below, we plot the first trial from each gesture to get a sense of this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e4dadb-5ad6-4717-a1f0-1b9e74c0a95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    2, 5,\n",
    "    figsize=(14, 6),\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    "    gridspec_kw=dict(hspace=0.3),\n",
    ")\n",
    "\n",
    "for gesture, ax in zip(GESTURE_NAMES, axes.ravel()):\n",
    "    single_trial_emg = get_single_trial_emg(emg, labels, gesture, 0)\n",
    "    plot_emg(single_trial_emg.T, linewidth=1, ax=ax)\n",
    "    ax.set_title(gesture)\n",
    "\n",
    "for ax in axes[:, 1:].ravel():\n",
    "    ax.tick_params(axis=\"y\", length=0)\n",
    "    ax.set_ylabel(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb4b968-b9f8-4f1c-a2ff-5f4deb5a7065",
   "metadata": {},
   "source": [
    "We note a few observations:\n",
    "* During periods of high activity, **the EMG signal is very \"spiky\"**, with rapid positive and negative deflections. This is the hallmark of many motor unit action potentials being detected by the electrodes.\n",
    "* **_Which_ electrode are active differs between gestures**. For example, electrode 9 is highly active during the \"Down\" gesture, but almost completely silent during the \"Index finger pinch\" gesture\n",
    "* **_How_ active each electrode is differs between gestures**. For example, electrode 10 is active during both the \"Down\" and \"Power grasp\" gestures, but significantly more so during the \"Down\" gesture\n",
    "* **_When_ each electrode is active differs between gestures**. For example, during the \"Down\" gesture electrode 6 becomes briefly active at around the 0.75 sec mark and then becomes silent, whereas in the \"Index finger pinch\" gesture it remains active throughout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb5549a-7850-4034-931b-0689b63eaae2",
   "metadata": {},
   "source": [
    "### 2.3. Nuisance variables\n",
    "\n",
    "Lets now go back and consider our goal of building an EMG-based gesture detector that can determine which EMG snippet was evoked by which gesture.\n",
    "\n",
    "Such an algorithm would have to be able to do two things:\n",
    "1. **detect** features of the EMG signal that differ between trials of _different_ gestures, such as those we noted above\n",
    "2. **ignore** features of the signal that differ between trials of the _same_ gesture, so that these trials are still classified as the same gesture.\n",
    "\n",
    "We call these latter features **nuisance variables**, since they are components of the signal that are unrelated to the information we want to extract from it; in other words, they're a nuisance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6214911f-6854-43f7-8a3b-78bee305c9e2",
   "metadata": {},
   "source": [
    "### 2.4. Compare EMG evoked by different trials of the same gesture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccade88-4ee8-48ce-9bc0-dd333fe9861a",
   "metadata": {},
   "source": [
    "To get some intuition for what the nuisance variables are in this problem, we visualize multiple trials of the same gesture. Below we plot the first 10 trials of the \"Splay\" gesture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d3ce7b-e93b-4190-bc24-ed9fac28c3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    2, 5,\n",
    "    figsize=(14, 6),\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    "    gridspec_kw=dict(hspace=0.3),\n",
    ")\n",
    "\n",
    "gesture = \"Splay\"\n",
    "\n",
    "for trial_index, ax in enumerate(axes.ravel()):\n",
    "    single_trial_emg = get_single_trial_emg(emg, labels, gesture, trial_index)\n",
    "    plot_emg(single_trial_emg.T, linewidth=1, ax=ax)\n",
    "    ax.set_title(f\"{gesture} trial {trial_index}\")\n",
    "\n",
    "for ax in axes[:, 1:].ravel():\n",
    "    ax.tick_params(axis=\"y\", length=0)\n",
    "    ax.set_ylabel(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855636b3-f0b3-418e-9895-0a919c684d85",
   "metadata": {},
   "source": [
    "First, note that electrodes 0-2, 6-7 and 9-10 are consistently active in all trials. That's good news: _which_ EMG electrodes are active is not just a feature that differs across trials from _different_ gestures, it is also a feature that remains mostly the same across trials of the _same_ gesture. This should therefore be a reliable feature to use for our gesture decoder.\n",
    "\n",
    "A feature that does _not_ seem to be so reliable is the precise timing of the spikes we see in each electrode. Look, for example, a the spikes in electrode 10. We see that in almost every trial, spikes begin in this electrode at around the 1 sec mark. But after this point, how many spikes occur and precisely when they occur seems to vary quite a bit from trial to trial. To see this more clearly, below we plot the activity in electrode 10 in the first 7 trials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f6b506-5907-4354-a7ef-9eb030cc101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 7\n",
    "electrode_index = 10\n",
    "spacing = 0.3\n",
    "\n",
    "single_electrode_activity_per_trial = [\n",
    "    get_single_trial_emg(emg, labels, gesture, trial_index)[:, electrode_index]\n",
    "    for trial_index in range(num_trials)\n",
    "]\n",
    "single_electrode_activity_per_trial = np.stack(single_electrode_activity_per_trial)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 3))\n",
    "\n",
    "plot_stacked_signals(\n",
    "    single_electrode_activity_per_trial,\n",
    "    xaxis=np.arange(emg.shape[1]) / EMG_SAMPLING_RATE_HZ,\n",
    "    labels_prefix=\"trial\",\n",
    "    cmap=\"Dark2\",\n",
    "    spacing=spacing,\n",
    "    linewidth=1,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_yticks(np.arange(num_trials) * spacing)\n",
    "ax.set_yticklabels(np.arange(num_trials))\n",
    "sns.despine(ax=ax, left=True)\n",
    "ax.set_xlabel(\"time (sec)\")\n",
    "ax.set_ylabel(\"trial\")\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1, 1), frameon=False)\n",
    "\n",
    "ax.set_title(f\"{gesture} gesture, channel {electrode_index}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4e3f2e-0c5e-413b-a264-50fe80a7b8e2",
   "metadata": {},
   "source": [
    "In every one of these trials, spikes begin to appear in this electrode after about 1 second. But in each case we see a different number of spikes, and the spikes never occur at exactly the same times as in the other trials. Note also that the shapes of the spikes differ substantially from trial to trial: sometimes the spikes are smaller, sometimes they're bigger, sometimes big spikes follow small spikes, ...\n",
    "\n",
    "These observations reveal the existence of a few nuisance variables:\n",
    "* the spike count in each electrode\n",
    "* millisecond-by-millisecond timing of spikes in each electrode\n",
    "* shape of the spikes in each electrode\n",
    "\n",
    "Why do these features vary so much from trial to trial? This is likely because, even though the same gesture is being executed on each trial, it may be executed in a slightly different way each time. For example, the subject might execute it faster, slower, harder, or softer on one trial than in another. Because behavior is directly controlled by motor units, these small differences in behavior correspond to differences in which motor units are activated when. This means that each trial contains different sequences of motor unit action potentials, which manifests itself in the surface EMG signal as differences in the motor unit action potential spikes picked up by each electrode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3b5227-5b43-4893-b003-8cc690676760",
   "metadata": {},
   "source": [
    "## 3. Preprocessing EMG\n",
    "\n",
    "Putting all of the above together, for our gesture decoder to tell apart _different_ gestures, it should be sensitive to:\n",
    "* which electrodes are active\n",
    "* how active each electrode is\n",
    "* when each electrode is active, on a coarse second-by-second scale\n",
    "\n",
    "On the other hand, for our gesture decoder to classify different trials of the _same_ gesture as the same gesture, it should be _in_sensitive to:\n",
    "* how many spikes occur in each electrode\n",
    "* when the spikes occur, on a fine millisecond-by-millisecond scale\n",
    "* the shapes of each spike\n",
    "\n",
    "To directly build these features into our gesture decoder, we will **preprocess** the EMG signal into a set of **EMG features** that have these properties baked in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463060c0-c50c-485e-987d-d9ea5b932e76",
   "metadata": {},
   "source": [
    "### 3.1. EMG signal power\n",
    "\n",
    "Specifically, we will use what is sometimes called the root-mean-square (RMS) power of the EMG signal, which consists of the following two steps:\n",
    "1. square the signal, thus making it all positive\n",
    "2. average it over a medium-sized time window (we will use 250 ms)\n",
    "3. take the square root\n",
    "\n",
    "Steps 1 and 2 help remove the nuisance variables from the signal, since we remove all millisecond-level spike timing and spike shape information by averaging the signal over a 250ms time window. Note that the positive and negative deflections of the signal would cancel each other out in an average, so we make sure to square the signal first in order to keep information about which electrodes are active and how active they are. Finally, the square root simply helps to keep the signal within a reasonable range, since squaring numbers can sometimes make them too big or too small.\n",
    "\n",
    "We also don't need to know what happens at every single 250 ms window in the trial, since the features we care about vary on a coarse second-by-second timescale. We therefore compute these RMS power features on successive 250 ms windows separated, or _strided_, by 50ms.\n",
    "\n",
    "We call these **RMS power EMG features**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c555a360-bdad-45ac-9086-83c45a0df72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "\n",
    "def compute_strided_features(\n",
    "    data: np.ndarray,\n",
    "    feature_fn: Callable[[np.ndarray], np.ndarray],\n",
    "    window_length: int,\n",
    "    stride: int\n",
    ") -> np.ndarray:\n",
    "    features = []\n",
    "    num_total_samples = data.shape[0]\n",
    "    for i in range(0, num_total_samples - window_length, stride):\n",
    "        window = data[i:(i + window_length)]\n",
    "        features.append(feature_fn(window))\n",
    "    features = np.stack(features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f565c49f-d01c-4d1f-86cc-8b3fc808a427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rms_power(data: np.ndarray) -> np.ndarray:\n",
    "    squared_power = np.sum(data** 2, axis=0)\n",
    "    rms_power = np.sqrt(squared_power)\n",
    "    return rms_power\n",
    "\n",
    "rms_power_window_length = 500  # 0.25 sec @ 2000Hz\n",
    "rms_power_stride = 100  # 0.05 sec @ 2000Hz\n",
    "\n",
    "rms_power_features = np.stack([\n",
    "    compute_strided_features(\n",
    "        x,\n",
    "        compute_rms_power,\n",
    "        window_length=rms_power_window_length,\n",
    "        stride=rms_power_stride,\n",
    "    )\n",
    "    for x in emg\n",
    "])\n",
    "\n",
    "rms_power_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953dc350-e9b8-46bb-9c35-0b3caad4dc90",
   "metadata": {},
   "source": [
    "Below, we plot the raw EMG from 5 trials and below each panel we plot the corresponding strided RMS power EMG features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aba4ff-3140-4018-b57e-a4bfa91611fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "gestures = ['Down', 'Left', 'Index point', 'Right', 'Splay']\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    2, 5,\n",
    "    figsize=(14, 6),\n",
    "    sharex=\"row\",\n",
    "    sharey=\"row\",\n",
    "    gridspec_kw=dict(hspace=0.3),\n",
    ")\n",
    "\n",
    "for i, gesture in enumerate(gestures):\n",
    "    \n",
    "    ax = axes[0, i]\n",
    "    raw_emg = get_single_trial_emg(emg, labels, gesture, 0)\n",
    "    plot_emg(raw_emg.T, linewidth=1, ax=ax)\n",
    "\n",
    "    ax = axes[1, i]\n",
    "    emg_features = get_single_trial_emg(rms_power_features, labels, gesture, 0)\n",
    "    plot_emg(\n",
    "        emg_features.T,\n",
    "        spacing=0.,\n",
    "        sampling_rate=EMG_SAMPLING_RATE_HZ / rms_power_stride,\n",
    "        linewidth=2,\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    axes[0, i].set_title(gesture)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b90a51-59fc-4778-9d30-c7717593c387",
   "metadata": {},
   "source": [
    "Note how the EMG signal power features do indeed show _which_ electrodes are active, _how_ active they are, and _when_ they are active, without being sensitive to the precise spike times and spike shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050fa7f0-ec9c-4c19-b1a3-1713968ad7b8",
   "metadata": {},
   "source": [
    "### 3.2. Visualizing all trials with dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd6f23b-2ba2-49d4-8ebc-7acc8dbf2796",
   "metadata": {},
   "source": [
    "The above plots are nice to see, to confirm our intuition that RMS power features bring forth differences between trials from different gestures. But it is hard to look at more than 5-10 trials at a time in this way. It would be ideal if we could look at _all_ the trials from _all_ the gestures. But we have 360 trials -- how can we visualize that many?\n",
    "\n",
    "For this, we use **dimensionality reduction** to plot each trial as a single point in a two-dimensional space. We first \"flatten\" out the time and electrode dimensions into one axis, turning each of our `time x electrode` timeseries into a single `time * electrode`-dimensional vector, in which each dimension corresponds to the RMS power of one electrode at one time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1219f3-019e-465a-9188-814527ed83de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMS power feature timeseries shape: \", rms_power_features.shape)\n",
    "\n",
    "rms_power_feature_vectors = rms_power_features.reshape(rms_power_features.shape[0], -1)\n",
    "\n",
    "print(\"RMS power flattened feature vectors shape: \", rms_power_feature_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bea1655-f638-4a5e-ba0b-a1063000b025",
   "metadata": {},
   "source": [
    "We can then apply PCA to reduce the dimensionality from 420 to 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ef67a6-b46e-4028-abe7-73251695afc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_projection = pca.fit_transform(X=rms_power_feature_vectors)\n",
    "\n",
    "print(\"RMS power PCA projection shape: \", pca_projection.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dbf200-82f3-49f1-bc80-94809a593504",
   "metadata": {},
   "source": [
    "Below, we plot each of these dimensionality-reduced trials as a single point in PC1-PC2 space, colored by which gesture was executed in that trial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f944160-326a-4dfa-b388-e69cd98b91bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=pca_projection[:, 0],\n",
    "    y=pca_projection[:, 1],\n",
    "    hue=labels,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"PC 1\")\n",
    "ax.set_ylabel(\"PC 2\")\n",
    "\n",
    "sns.despine(ax=ax)\n",
    "\n",
    "ax.legend(loc=\"upper left\", bbox_to_anchor=(1, 1), frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031eecb7-73da-44ff-979f-d5791de4c327",
   "metadata": {},
   "source": [
    "It is hard to see much structure here, mainly because of the presence of one strong outlier that has a very high projection along PC2. Because PCA just tries to capture as much variance as possible in the data, it is known to be sensitive to such outliers.\n",
    "\n",
    "We therefore next try a different method called [t-SNE](https://scikit-learn.org/stable/modules/manifold.html#t-distributed-stochastic-neighbor-embedding-t-sne), which is a non-linear dimensionality reduction technique that is typically less sensitive to outliers and very sensitive to cluster-like structure in the data. This is a very popular tool for dimensionality reduction, but somewhat less interpretable than PCA due to its higher complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908eea6e-46e3-4a09-988d-18b915d9ad8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "tsne_projection = tsne.fit_transform(X=rms_power_feature_vectors)\n",
    "\n",
    "print(\"RMS power t-SNE projection shape: \", tsne_projection.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f06ebba-20d5-4105-a67a-28b164bcd1d8",
   "metadata": {},
   "source": [
    "As we did with PCA above, below we plot the t-SNE projections of every trial, colored by gesture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077b1a18-149d-4f21-a237-aeaf5198053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "sns.scatterplot(\n",
    "    x=tsne_projection[:, 0],\n",
    "    y=tsne_projection[:, 1],\n",
    "    hue=labels,\n",
    "    ax=ax\n",
    ")\n",
    "sns.despine(ax=ax)\n",
    "ax.legend(loc=\"upper left\", bbox_to_anchor=(1, 1), frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99216f42-54cd-4ada-bd4f-e849c0fc4f50",
   "metadata": {},
   "source": [
    "Here we see a lot of very clear structure! Note that all the trials from a given gesture tend to cluster close together, and each gesture's cluster is overlaps little with the clusters of other gestures. This shows that our RMS power features are indeed sensitive to differences between gestures, while not being overly sensitive to differences between trials of the same gesture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a368c9d-148a-4ff8-9033-4d63802a66ae",
   "metadata": {},
   "source": [
    "## 4. Classifying gestures from RMS power features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee596e4-6b23-4ef9-9358-91c609ee33da",
   "metadata": {},
   "source": [
    "Now satisfied with our preprocessed EMG features, we finally move on to the last step of constructing our gesture decoder: machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3f1f4f-f2f6-406a-963d-ec36aa31c589",
   "metadata": {},
   "source": [
    "### 4.1. Fit a logistic regression model\n",
    "\n",
    "We will use the data available to use to train a simple linear classifier called a **[logistic regression model](https://en.wikipedia.org/wiki/Multinomial_logistic_regression#As_a_log-linear_model)**, which takes as input a trial of EMG data, $X$, and predicts the probability that that EMG signal was produced by each of the 10 gestures as follows:\n",
    "$$\n",
    "p(\\text{single trial EMG data } X \\text{ is gesture } k) = \\frac{1}{Z} e^{\\sum_{i=1}^N \\sum_{t=1}^T w^{(k)}_{it} X_{it}}\n",
    "$$\n",
    "where $X_{it}$ is the RMS power in electrode $i$ at time $t$, $N$ is the total number of electrodes, $T$ is the total number of RMS power feature samples in each trial, and $K$ is the number of gestures. The exponentiation is to turn the weighted linear sum of EMG activity -- which can be negative or positive -- into a positive number to make it into a probability, and $Z$ is a normalization constant to make sure these probabilities are between 0 and 1 and sum to 1 (set to $Z = \\sum_{k' = 1}^K e^{\\sum_{i=1}^N \\sum_{t=1}^T w^{(k')}_{it} X_{it}}$). The model's _weights_, $w^{(k)}_{it}$, are the parameters that we optimize to fit the data.\n",
    "\n",
    "We first fit these in just a couple of lines of code using the `sklearn.linear_model.LogisticRegression` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f07cd2-c0fe-48d5-8197-850abfd9e168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(\n",
    "    penalty=None,\n",
    "    multi_class=\"multinomial\",\n",
    "    max_iter=2000,\n",
    ")\n",
    "\n",
    "model.fit(X=rms_power_feature_vectors, y=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cc773a-e006-49d1-bb32-583bc44c634d",
   "metadata": {},
   "source": [
    "We then evaluate the fitted model by calculating the accuracy of its predictions, i.e. the percent of trials that whose gesture was correctly predicted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be5b63d-591c-42ac-8aa4-bff4555339c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = model.score(X=rms_power_feature_vectors, y=labels)\n",
    "\n",
    "print(\"accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8970765a-5340-48b6-b819-8234496e2bef",
   "metadata": {},
   "source": [
    "A perfect score! But this is hard to believe: there is noise in the data (e.g. that outlier in the PCA plot above) that should preclude us from being able to predict each gesture perfectly.\n",
    "\n",
    "This suspiciously good result suggests that our model may have **overfit its training data**: it has learned to use very specific features of these data in order to make its predictions, but these features may not exist in new trials of EMG outside of these data. Note that, for this model to be useful as a gesture decoder in a neuromotor interface, we want it to work for executions of these gestures in real-life scenarios with a human-computer interface. Will the model actually correctly classify EMG data from new trials of these gestures? Or can it only correctly classify the 360 pre-recorded trials it was trained on?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0f7f22-683f-44d0-b04a-20c782e72d0c",
   "metadata": {},
   "source": [
    "### 4.2. Cross-validation\n",
    "\n",
    "To assess this, we perform **cross-validation**: we remove a few trials from the data and, after fitting the model to the remaining data (which we call the **training set**), we use these held-out trials (which we call the **test set**) to assess whether the model can correctly classify trials outside of the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e673045-160d-4eb1-9906-7630d5336a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_to_hold_out = 10\n",
    "\n",
    "X_test = rms_power_feature_vectors[:num_samples_to_hold_out]\n",
    "y_test = labels[:num_samples_to_hold_out]\n",
    "\n",
    "X_train = rms_power_feature_vectors[num_samples_to_hold_out:]\n",
    "\n",
    "y_train = labels[num_samples_to_hold_out:]\n",
    "\n",
    "model.fit(X=X_train, y=y_train)\n",
    "\n",
    "print(\"train set accuracy:\", model.score(X=X_train, y=y_train))\n",
    "print(\"test set accuracy:\", model.score(X=X_test, y=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af62990-41f0-4d25-91c8-93f36544bae1",
   "metadata": {},
   "source": [
    "We see that indeed, despite performing again perfectly on the train set, the model performs significantly worse on the held-out test set. This a clear sign that the model is overfitting its training data.\n",
    "\n",
    "Is this a general property of this problem, or did we maybe get unlucky with some particularly difficult examples in the small test set? Before overreacting, let's first look closely at what the labels were in the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72358aa-6aa8-4b96-a06d-37601cf85c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f1d13c-9b78-46c0-984e-73383080848d",
   "metadata": {},
   "source": [
    "Note that there were more of one `Middle finger pinch` and `Index finger pinch` in the test set, which means that there were fewer in the train set.\n",
    "\n",
    "To correct for this and ensure that there are the same number of samples of each gesture in the train and test sets, we use the `StratifiedKFold` cross validation splitter from `sklearn`, which specifically enforces that each train/test **split** of the data contains even numbers of samples from each class in the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027cb998-6ba6-4c72-97a2-3011d680dd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "cv = StratifiedKFold(\n",
    "    n_splits=9,\n",
    "    shuffle=True,\n",
    "    random_state=24\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1478774f-5740-4cb2-939f-d809258af89b",
   "metadata": {},
   "source": [
    "The `.split` method of this class will produce `n_splits` train/test splits as follows:\n",
    "1. split up the data into `n_splits` equally sized partitions\n",
    "2. ensure that each partition contains the same number of samples from each class\n",
    "3. produce `n_splits` train/test splits, in which each one a different one of the partitions is used as the test set and the remaining ones are put in the training set\n",
    "\n",
    "We have set `n_splits=9`, so we will get 9 different train/test splits. We confirm below that each one of these splits contains even number of samples from each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f7160a-a3f1-48c3-8592-37177dca55a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_splits = cv.split(rms_power_feature_vectors, labels)\n",
    "\n",
    "for split_num, (train_indices, test_indices) in enumerate(train_test_splits):\n",
    "    classes, counts = np.unique(labels[train_indices], return_counts=True)\n",
    "    print(f\"split {split_num}:\")\n",
    "    print(\"\\ttrain set contains following class counts:\", counts)\n",
    "    classes, counts = np.unique(labels[test_indices], return_counts=True)\n",
    "    print(\"\\ttest set contains following class counts:\", counts)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f30aed-867b-49e1-8f63-1c55e5be2c0b",
   "metadata": {},
   "source": [
    "Now we can loop over each one of these train/test splits, evaluate the training score and test score, and see if the model is overfitting in every single case or whether the overfitting we saw above was more of an isolated incident due to the class imbalance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0921a10-fb8b-4953-909a-ea916be12ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble train/test sets in each split\n",
    "train_test_splits = cv.split(rms_power_feature_vectors, labels)\n",
    "\n",
    "# loop over train/test sets and calculate train/test score for each one\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for train_inds, test_inds in train_test_splits:\n",
    "\n",
    "    # get train set\n",
    "    X_train = rms_power_feature_vectors[train_inds]\n",
    "    y_train = labels[train_inds]\n",
    "\n",
    "    # get test set\n",
    "    X_test = rms_power_feature_vectors[test_inds]\n",
    "    y_test = labels[test_inds]\n",
    "\n",
    "    # fit model to train set\n",
    "    model.fit(X=X_train, y=y_train)\n",
    "\n",
    "    # assess model on test set\n",
    "    train_score = model.score(X=X_train, y=y_train)\n",
    "    train_scores.append(train_score)\n",
    "    test_score = model.score(X=X_test, y=y_test)\n",
    "    test_scores.append(test_score)\n",
    "\n",
    "# take the mean over all train/test sets\n",
    "train_score = np.mean(train_scores)\n",
    "test_score = np.mean(test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2047b06e-7e2d-40f8-ac6e-26e376957474",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "sns.barplot(\n",
    "    x=[\"train\", \"test\"],\n",
    "    y=[train_score, test_score],\n",
    "    hue=[\"train\", \"test\"],\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "for i, x in enumerate([train_score, test_score]):\n",
    "    ax.annotate(\n",
    "        f\"{100 * x:.1f}%\",\n",
    "        (i, x - .02),\n",
    "        ha=\"center\",\n",
    "        va=\"top\",\n",
    "        color=\"w\",\n",
    "    )\n",
    "\n",
    "ax.set_ylim([0.5, 1.01])\n",
    "ax.grid(axis=\"y\", zorder=-1)\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "ax.set_xlabel(\"train/test split\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "\n",
    "sns.despine(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af85396-9417-4f27-84dc-ca7827881471",
   "metadata": {},
   "source": [
    "We see that there remains a substantial **train/test gap**, although we're still getting a pretty good average accuracy on the held-out test sets. So maybe the overfitting is not as bad as we originally thought.\n",
    "\n",
    "How can we reduce overfitting? One way to do this is to add a **regularization penalty** that penalizes large weights, thus restricting the weights to be small and restricting the model's capacity to overfit the training data. We can do this in sklearn by setting `penalty=\"l2\"` and setting a small value for `C`, which is the inverse of the importance given to the regularization penalty. Here we use `C = 2.0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f3c498-ce07-44a8-ac6b-436f2b817973",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(\n",
    "    penalty=\"l2\",\n",
    "    C=3.0,\n",
    "    multi_class=\"multinomial\",\n",
    "    max_iter=5000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d204d530-3142-4709-a9c8-5e84e52a75a9",
   "metadata": {},
   "source": [
    "To take care of the cross-validation loop we implemented above, we use the function `cross_validate`, which essentially does exactly the same thing, and returns the train and test scores for each split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fc41b2-12bf-45a5-b4aa-3f15880e8c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scores = cross_validate(\n",
    "    model,\n",
    "    X=rms_power_feature_vectors,\n",
    "    y=labels,\n",
    "    cv=cv,\n",
    "    return_train_score=True,\n",
    ")\n",
    "\n",
    "train_score = np.mean(scores[\"train_score\"])\n",
    "test_score = np.mean(scores[\"test_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5fd401-7957-433e-8ecd-66607c183a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "sns.barplot(\n",
    "    x=[\"train\", \"test\"],\n",
    "    y=[train_score, test_score],\n",
    "    hue=[\"train\", \"test\"],\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "for i, x in enumerate([train_score, test_score]):\n",
    "    ax.annotate(\n",
    "        f\"{100 * x:.1f}%\",\n",
    "        (i, x - .02),\n",
    "        ha=\"center\",\n",
    "        va=\"top\",\n",
    "        color=\"w\",\n",
    "    )\n",
    "\n",
    "ax.set_ylim([0.5, 1.01])\n",
    "ax.grid(axis=\"y\", zorder=-1)\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "ax.set_xlabel(\"train/test split\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "\n",
    "sns.despine(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282b75a4-a4a6-4312-8627-d8f638746656",
   "metadata": {},
   "source": [
    "Our test set performance has now improved. The regularization penalty seems to have indeed improved the model's ability to classify gestures outside of the training set. Note that the train score has gone down, also because of the regularization. But that doesn't matter: the important thing is that the test score has risen, as this is what matters for a gesture decoder to be useable in real life situations, which are outside of the training set!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8de756a-ea82-4e70-97e8-4304371f384a",
   "metadata": {},
   "source": [
    "### 4.3. Examining model confusions\n",
    "\n",
    "Unfortunately the gesture decoder still is not perfect, as it gets about 9% of its predictions wrong. For the purposes of deploying this gesture decoder for an human-computer interface, it would be useful to know whether the mistakes it makes are more common for some gestures for others. For example, \n",
    "* maybe it gets \"Down\" right every time, but makes more mistakes with the \"Up\" gesture. In this case, we might want to remove the \"Up\" gesture entirely to avoid mistakes when using the interface.\n",
    "* another possibility is that maybe \"Down\" and \"Left\" are confused often, since they involve similar deflections of the wrist. In that case, we might want to combine these two gestures into one gesture, which might be correctly classified more often\n",
    "\n",
    "To investigate these possibilities, we compute the **confusion matrix**, which shows what percentage of the time each gesture is confused with another gesture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d29e9ac-478e-42e7-b39e-f31f70452351",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Function to evaluate confusion matrix on\n",
    "held-out test sets from cross-validation splits\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "\n",
    "def compute_confusion_matrix(\n",
    "    data: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    model: BaseEstimator,\n",
    "    cv: BaseCrossValidator,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "        \n",
    "    # assemble train/test splits\n",
    "    train_test_splits = cv.split(data, labels)\n",
    "    \n",
    "    # loop over train/test splits and compute test set predictions in each case\n",
    "    \n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    \n",
    "    for train_inds, test_inds in train_test_splits:\n",
    "    \n",
    "        # assemble train set\n",
    "        X_train = data[train_inds]\n",
    "        y_train = labels[train_inds]\n",
    "    \n",
    "        # assemble test set\n",
    "        X_test = data[test_inds]\n",
    "        y_test = labels[test_inds]\n",
    "    \n",
    "        # fit model to train set\n",
    "        model.fit(X=X_train, y=y_train)\n",
    "    \n",
    "        # compute model predictions on test set\n",
    "        y_pred.append(model.predict(X=X_test))\n",
    "    \n",
    "        # grab test set true labels\n",
    "        y_true.append(y_test)\n",
    "    \n",
    "    # aggregate predictions and true labels from all test\n",
    "    # sets into a single numpy array\n",
    "    y_pred = np.concatenate(y_pred, axis=0)\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "    \n",
    "    # compute confusion matrix over all predictions\n",
    "    cmat = confusion_matrix(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        labels=GESTURE_NAMES,\n",
    "        normalize=\"true\",\n",
    "    )\n",
    "\n",
    "    # assemble confusion matrix into a dataframe with gesture names\n",
    "    cmat = pd.DataFrame(\n",
    "        cmat,\n",
    "        index=GESTURE_NAMES,\n",
    "        columns=GESTURE_NAMES,\n",
    "    )\n",
    "\n",
    "    return cmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b80e99-cdf6-4d4c-a8e2-3dd036681939",
   "metadata": {},
   "outputs": [],
   "source": [
    "rms_power_confusion_matrix = compute_confusion_matrix(\n",
    "    data=rms_power_feature_vectors,\n",
    "    labels=labels,\n",
    "    model=model,\n",
    "    cv=cv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8217cb5b-e869-4781-9602-7e7155ec4c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "\n",
    "ax = sns.heatmap(\n",
    "    np.around(rms_power_confusion_matrix, decimals=2),\n",
    "    vmin=0.,\n",
    "    vmax=1.,\n",
    "    cmap=\"Reds\",\n",
    "    ax=ax,\n",
    "    annot=True,\n",
    "    annot_kws=dict(fontsize=10),\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"predicted gesture\");\n",
    "ax.set_ylabel(\"true label\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ff1cb4-4ace-44fc-b83e-7dde699d8320",
   "metadata": {},
   "source": [
    "Each row of this matrix shows the percentage of times that a given gesture was classified as another gesture.\n",
    "\n",
    "For example, if we look at the fourth row, most of the misclassified \"Middle finger pinch\" trials are confused with \"Two finger pinch\". This is not too surprising, since these two gestures are quite similar in that they both involve squeezing your middle finger and thumb together: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d64e97-d327-4531-9e2d-c313244cc699",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"./pinches.png\", width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6025ce45-2a90-4d10-b9df-4f06586e7f6e",
   "metadata": {},
   "source": [
    "For the same reason, the \"Two finger pinch\" gesture gets confused often with both \"Middle finger pinch\" and \"Index finger pinch\". We might therefore consider removing the \"Two finger pinch\" entirely, and reducing the total number of gestures to 9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f5db4d-ac96-4abd-810b-f3cfad0e01ca",
   "metadata": {},
   "source": [
    "Another observation is that \"Right\" is often confused with \"Up\". This is also not too surprising, since both of these gestures involve a deflection of the wrist in a similar direction, but in different postures:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c200df2-33f3-4c51-875b-019c6fb7c4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"./right-up.png\", width=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a865e31c-ca33-48da-b085-7e3e7d38661b",
   "metadata": {},
   "source": [
    "We might therefore consider combining \"Right\" and \"Up\" into one gesture that is more reliably detected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6036303b-29c7-434b-a75f-6c240cad0096",
   "metadata": {},
   "source": [
    "## 5. [bonus] Spectrogram features\n",
    "\n",
    "Above we decided to remove millisecond-by-millisecond fluctuations in the signal by using RMS power EMG features, which are an average of the signal over strided 250ms windows. We reasoned that this would be a good idea because these fine timescale features of the signal were mostly nuisance variables.\n",
    "\n",
    "But did we potentially throw out valuable information too? One way to bring these back into the picture is to use **spectrogram features**, which calculate the power in the signal within different frequency bands by using the [discrete Fourier transform](https://en.wikipedia.org/wiki/Discrete_Fourier_transform), for each electrode. The discrete Fourier transform within each 250ms window will have information about the overall power within that window, but also have information about how that power is distributed across low (coarse timescale) and high (fine timescale) frequency bands.\n",
    "\n",
    "Can incorporating this additional information in our EMG features improve the performance of our gesture decoder?\n",
    "\n",
    "We begin by computing the spectrogram features for each trial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484410a2-ef01-4f14-af12-e0334392292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal.windows import hann, hamming\n",
    "\n",
    "NFFT = 64\n",
    "\n",
    "def compute_power_spectrum(x: np.ndarray) -> np.ndarray:\n",
    "    fft = np.fft.rfft(x, n=NFFT, axis=0)\n",
    "    power = np.abs(fft)\n",
    "    return power\n",
    "\n",
    "spectrogram_features = np.stack([\n",
    "    compute_strided_features(\n",
    "        x,\n",
    "        compute_power_spectrum,\n",
    "        window_length=rms_power_window_length,  # use same 250ms window length that we used for RMS power features\n",
    "        stride=rms_power_stride,  # use same 50ms stride that we used for RMS power features\n",
    "    )\n",
    "    for x in emg\n",
    "])\n",
    "\n",
    "print(\"spectrogram feature timeseries shape: \", spectrogram_features.shape)\n",
    "\n",
    "spectrogram_feature_vectors = spectrogram_features.reshape(spectrogram_features.shape[0], -1)\n",
    "\n",
    "print(\"spectrogram flattened feature vectors shape: \", spectrogram_feature_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26088c2-81cc-485b-aae5-7bb616db8e50",
   "metadata": {},
   "source": [
    "Note that, because we are now separating the signal power in each electrode _and_ in each frequency, these features are now much higher-dimensional than the RMS power features: 13860-dimensional instead of 420-dimensional.\n",
    "\n",
    "To first get a rough sense of whether these features contain the information we need, we run t-SNE to visualize each trial, as we did above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a9ee2f-6048-4f41-8750-c3bcc80369c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "tsne_projection = tsne.fit_transform(X=spectrogram_feature_vectors)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "sns.scatterplot(\n",
    "    x=tsne_projection[:, 0],\n",
    "    y=tsne_projection[:, 1],\n",
    "    hue=labels,\n",
    "    ax=ax\n",
    ")\n",
    "sns.despine(ax=ax)\n",
    "ax.legend(loc=\"upper left\", bbox_to_anchor=(1, 1), frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5461d1-74b6-480d-b499-23a6a7c2a116",
   "metadata": {},
   "source": [
    "We again see a pretty nice clustering of the various gestures. We still see the issue we noted with our RMS power features gesture decoder, where the \"Up\" and \"Right\" gestures overlap, and the three pinch gestures overlap.\n",
    "\n",
    "Next, we set up our cross-validation and train a logistic regression model, with regularization. In this case, we use a higher regularization penalty (smaller `C`, which is inversely related to the strenght of the regularization penalty) to make sure we don't overfit to the higher-dimensional features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61d9825-3872-4c46-b794-278dbd7dc120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# build cross-validation splitter\n",
    "cv = StratifiedKFold(\n",
    "    n_splits=9,\n",
    "    shuffle=True,\n",
    "    random_state=24\n",
    ")\n",
    "\n",
    "# set up model\n",
    "model = LogisticRegression(\n",
    "    penalty=\"l2\",\n",
    "    C=1.0,\n",
    "    multi_class=\"multinomial\",\n",
    "    max_iter=5000,\n",
    ")\n",
    "\n",
    "# fit and evaluate model with cross-validation\n",
    "scores = cross_validate(\n",
    "    model,\n",
    "    X=spectrogram_feature_vectors,\n",
    "    y=labels,\n",
    "    cv=cv,\n",
    "    return_train_score=True,\n",
    ")\n",
    "\n",
    "# average train and test scores over all train/test splits\n",
    "train_score = np.mean(scores[\"train_score\"])\n",
    "test_score = np.mean(scores[\"test_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d207e984-9eb0-4d89-8d99-da27391d75b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "sns.barplot(\n",
    "    x=[\"train\", \"test\"],\n",
    "    y=[train_score, test_score],\n",
    "    hue=[\"train\", \"test\"],\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "for i, x in enumerate([train_score, test_score]):\n",
    "    ax.annotate(\n",
    "        f\"{100 * x:.1f}%\",\n",
    "        (i, x - .02),\n",
    "        ha=\"center\",\n",
    "        va=\"top\",\n",
    "        color=\"w\",\n",
    "    )\n",
    "\n",
    "ax.set_ylim([0.5, 1.01])\n",
    "ax.grid(axis=\"y\", zorder=-1)\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "ax.set_xlabel(\"train/test split\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "\n",
    "sns.despine(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d934685-d576-4d90-9c41-4ca19a091dd9",
   "metadata": {},
   "source": [
    "We see that the test scores is pretty similar to what we saw with RMS power features, which is consistent with our intuition that the fine timescale features of the signal are not too important for classifying gestures.\n",
    "\n",
    "Are they more important for some gestures than for others? To examine this, we evaluate the confusion matrix and plot it side-by-side with the confusion matrix for RMS power features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26fddce-7cef-474d-bb3e-908fdcba6de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram_confusion_matrix = compute_confusion_matrix(\n",
    "    data=spectrogram_feature_vectors,\n",
    "    labels=labels,\n",
    "    model=model,\n",
    "    cv=cv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9dd837-a9c5-48ea-bca5-794f92a5da1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    1, 2,\n",
    "    figsize=(12, 5),\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    ")\n",
    "\n",
    "for cmat, title, ax in zip(\n",
    "    [rms_power_confusion_matrix, spectrogram_confusion_matrix],\n",
    "    [\"RMS power features\", \"spectrogram features\"],\n",
    "    axes\n",
    "):\n",
    "    sns.heatmap(\n",
    "        np.around(cmat, decimals=2),\n",
    "        vmin=0.,\n",
    "        vmax=1.,\n",
    "        cmap=\"Reds\",\n",
    "        annot=True,\n",
    "        annot_kws=dict(fontsize=10),\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"predicted gesture\")\n",
    "    ax.set_ylabel(\"true label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b1e7ff-03d5-4a17-9968-8d8fa6cd28ce",
   "metadata": {},
   "source": [
    "We find some interesting diffrences, which reveal that the fine timescale features of the data are important for some gestures but nuisance variables for others:\n",
    "* \"Right\" and \"Up\" are much more confused when using spectrogram features, dropping from 89% accuracy to 64% accuracy\n",
    "* But, \"Two finger pinch\" classification gets a lot better, jumping up from 81% accuracy to 94% accuracy, and the \"Index finger pinch\" and \"Middle finger pinch\" accuracy also goes up slightly. Evidently, the fine timescale information contained in the spectorgram features allows the decoder to better tell apart the different pinch gestures.\n",
    "\n",
    "On conclusion we might draw from this is that, if we were to combine the \"Right\" and \"Up\" gestures into one, spectrogram features would lead to improved classification of the resulting 9 gestures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
